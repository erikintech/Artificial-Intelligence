{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3bf3ba-5343-420a-aedc-3bf3af9126ec",
   "metadata": {},
   "source": [
    "# **Features Engineering**\n",
    "\n",
    "- In machine learning algoritms, the performance of the model is depending on data preprocessing and data handling, so if we use feature engineering we can improve the performance of our model.\n",
    "- The main goal of Feature Engineering is to get the best results from the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4569b3-59c2-40a9-9f8e-eaad9022320c",
   "metadata": {},
   "source": [
    "## **Variable Transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14c7bd-1e18-4d1e-9b1d-a8b1fc7bcd2e",
   "metadata": {},
   "source": [
    "Predictions from linear regression models assume residualsare **normally distributed**\n",
    "\n",
    "Features and predicte data areoften **skewed** (distored away from the center).\n",
    "\n",
    "**Data transformation** can solve this issue\n",
    "\n",
    "<div align=center> <img src='./skeweddata.jpg'/> </div>\n",
    "\n",
    "**Statiscally**\n",
    "\n",
    "- **Positively Skewed:** Median < Mean\n",
    "- **Neagtively skewed:** Mean < Median\n",
    "- **Simetric distribution**: Mean = Median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86911d09-d9ed-4e21-8685-f01cfc11e92d",
   "metadata": {},
   "source": [
    "### **Transformations: Log Features**\n",
    "\n",
    "**Log transformation** can be useful for linear regression.\n",
    "\n",
    "$$\n",
    "    y_\\beta(x)\\ =\\ \\beta_0+\\beta_1log(x)\n",
    "$$\n",
    "\n",
    "The linear regression model involves linear combinations offeatures.\n",
    "\n",
    "To apply this transformation we can use numpy library with the log method, the transformation can be applied on each column with skewed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9f0926-8d68-4f8d-a45c-a95365307834",
   "metadata": {},
   "source": [
    "### **Transformations: Polynomial Features**\n",
    "\n",
    "We can estimate higher-order relationships in this data by adding polynomial features\n",
    "$$\n",
    "    y_\\beta(x)\\ =\\ \\beta_0+\\beta_1x+\\beta_2x^2\n",
    "$$\n",
    "This allow us to use the same linear model\n",
    "\n",
    "### **Polynomial Feature Syntax**\n",
    "\n",
    "```python\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    \n",
    "    polyFeat = PolynomialFeatures()\n",
    "    \n",
    "    polyfeat = polyFeat.fit(X_data)\n",
    "    X_poly = polyfeat.ttransform(X_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc1e71-25b8-45b6-80cb-a005ea31e761",
   "metadata": {},
   "source": [
    "## **Feature Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbba20c8-c874-47fb-94bc-56bb40610a2a",
   "metadata": {},
   "source": [
    "### **Variable Selection**\n",
    "\n",
    "**Variable selection** invilves choosing the set of features to include in the model.\n",
    "\n",
    "Variables must often be transfored before they can be included in models.\n",
    "\n",
    "In addition to log and polynomial tranformations, this caninvolve:\n",
    "- **Encoding:** converting non-numeric features to numeric features\n",
    "- **Scaling:** converting the scale of numeric data so they are comparable.\n",
    "\n",
    "The correct method of scaling or encoding depends on the type of feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a273c-1f81-4b4e-a386-f74880494222",
   "metadata": {},
   "source": [
    "### **Feature Encoding: Types of FEatures**\n",
    "**Encoding** is often applied to **categorical data**, that take non-numeric values, two primary types:\n",
    "\n",
    "- **Nominal:** categorical variables take values inunordered categories (eg: Red, blue, green; true, false)\n",
    "- **Ordinal:** categorical variables take values in ordered categories (eg: High, Medium, Low)\n",
    "\n",
    "### **Features Encoding: Approaches**\n",
    "\n",
    "- **Binary encoding:** converts variables to either 0 or 1\n",
    "- **One-Hot encoding:** converts variables that take multiple values into binary (0 or 1) variables\n",
    "- **Ordinal encoding:** involves converting ordered categories to numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70503319-cd36-47ae-9a74-32808545c7d0",
   "metadata": {},
   "source": [
    "## **Features Scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e1c044-77b6-4df5-ab05-6ea2e1e9bc22",
   "metadata": {},
   "source": [
    "## **Why should we use feature scaling?**\n",
    "\n",
    "Some ML algorithms are sensitive to feature scaling, and others are invariant to it.\n",
    "\n",
    "**Gradient Descent Based Algorithms**\n",
    "\n",
    "Algorithms like linear regression, logistic regression that use gradient descent as an optimization technique require data to be scaled.\n",
    "\n",
    "**Distance Based Algorithm**\n",
    "\n",
    "Algorithms like KNN, Support Vector MAchine use distances between data point to determine their similarity, for this reason when we use this type of algorithms is important that we use feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1642cb-66b9-44cb-9250-d237c3313a0d",
   "metadata": {},
   "source": [
    "## **Normalization**\n",
    "\n",
    "Normalization is a technique for scaling data in a range between 0 and 1, this is also know as **Min-Max Scaling**\n",
    "\n",
    "***Formula:***\n",
    "$$\n",
    " \\tilde X= \\frac{X - X_{min}}{X_{max} - X_{min} }\n",
    "$$\n",
    "\n",
    "- When the value of X is the minimun in the columns, the numerator will be 0 and $\\tilde X$ is 0\n",
    "- When X is the maximun value in  the column , the numerator is equal to the denominator and thus the value of $\\tilde X$ is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dada165-ccfd-48d2-80f9-abaaf3095022",
   "metadata": {},
   "source": [
    "## **Standarization**\n",
    "\n",
    "It is the other technique of scaling where the values are centered around the mean with a unit standard deviation.\n",
    "\n",
    "***Formula:***\n",
    "\n",
    "$$\n",
    "    \\grave X = \\frac{X - \\nu }{\\alpha}\n",
    "$$\n",
    "\n",
    "*Where:*\n",
    "\n",
    "- **$\\nu$** represents the mean of the feature values\n",
    "\n",
    "- **$\\alpha$** represents the standard deviation of te feature values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c98c21-9776-43e6-9d8b-02f105b6cd18",
   "metadata": {},
   "source": [
    "### **When should we use Normalization or Standarization?**\n",
    "\n",
    "- **Normalization** is used when you know that the distribution of your datadoes not follow a Gaussian distribution, in algorithms like K-Nearest Neighbors and Neural Networks.\n",
    "- **Standarization** is used when you know that your data follows a Gaussian distribution, standarization can help us specially when our data have outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2fa8cc-26c2-436e-baba-784002e90305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envML",
   "language": "python",
   "name": "envml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
